# Cyber Shujaa Data & AI – Week 11 NLP Assignment

## Project Title
**Sentence Similarity Using BERT and Cosine Similarity**

## Description
This project demonstrates how to use a pre-trained BERT model (`bert-base-uncased`) to compute semantic similarity between sentence pairs using contextual embeddings and cosine similarity. It is part of the Week 11 NLP module under the Cyber Shujaa Program.

The assignment includes:
- Tokenization and embedding extraction using Hugging Face Transformers
- Use of the `[CLS]` token for sentence-level representations
- Cosine similarity computation between BERT embeddings
- Label prediction based on similarity threshold (0.7)
- Evaluation of prediction accuracy
- Reflection on key NLP concepts including BERT, contextual embeddings, and the self-attention mechanism

## Files Included

| File | Description |
|------|-------------|
| `Week11_BERT_NLP_Assignment.ipynb` | Google Colab notebook with full implementation |
| `NLP_Assignment_Report.pdf` | PDF report with introduction, screenshots, results, and conclusion |

## View the Colab Notebook

You can view or run the notebook directly on Google Colab here:  
**[Colab Notebook Link]([https://colab.research.google.com/drive/1v6vtMjUiIwFScBQMJanYLYq5Fw-uoI6W?usp=sharing])**

## Requirements

This notebook was designed to run in Google Colab, which already includes:
- TensorFlow
- Hugging Face Transformers
- scikit-learn
- NumPy

No additional setup is required if run in Colab.

## Author

**Denis Kombe Karisa**  
Bachelor of Science in Information and Communication Technology  
Jaramogi Oginga Odinga University of Science and Technology  
Cyber Shujaa Data & AI – Week 11 Assignment  
August 2025

